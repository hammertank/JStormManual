# JStorm安装手册 #
## 适用版本 ##
JStorm 2.1.0
## 安装 ##
安装步骤参照[官方文档](https://github.com/alibaba/jstorm/wiki/%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85)
## 配置 ##
### 最简配置 ###
在安装包内storm.yaml基础上修改：

- storm.zookeeper.servers: 表示zookeeper 的地址

### 常用配置 ###
- storm.zookeeper.root: 表示JStorm在zookeeper中的根目录，当多个JStorm共享一个zookeeper时，需要设置该选项，默认即为“/jstorm”
- storm.local.dir: 表示JStorm临时数据存放目录，需要保证JStorm程序对该目录有写权限
- worker.memory.size: woker的默认内存（JVM heap）大小，单位：字节

### 资源隔离 ###
相关配置项：

- supervisor.enable.cgroup

参考：[资源硬隔离](https://github.com/alibaba/jstorm/wiki/%E8%B5%84%E6%BA%90%E7%A1%AC%E9%9A%94%E7%A6%BB)

### 资源分组 ###
相关配置项：

- nimbus.groupfile.path

如[官方wiki](https://github.com/alibaba/jstorm/wiki/%E8%B5%84%E6%BA%90%E5%88%86%E7%BB%84)所述，JStorm 自0.95之后已经不支持将一个集群的资源进行分组隔离。配置项`nimbus.groupfile.path`已经被从源码中删除，无任何作用。

### Nimbus HA ###
相关配置项：

- nimbus.host

在Jstorm中我们可以启动任意多个Nimbus，Nimbus进程启动后即通过抢占zookeeper的InterProcessMutex锁来竞争成为leader，没有抢到锁的非leader Nimbus进程一直处于block状态，不进行后续工作，当leader宕机时，抢占到锁的下一个Nimbus节点成为新leader，由此解决了Nimbus的单节点问题。

启动Nimbus有两种方式：

1. 直接使用`jstorm`命令：

	$ nohup jstorm nimbus & #nimbus.host配置项不影响nimbus的IP

2. 使用`$JSTORM_HOME/bin/start.sh`脚本：

	$ start.sh #需要将nimbus.host配置为本机IP地址（即`hostname -i`的值）


### Woker Slots ###
相关配置项：

- supervisor.slots.ports.base
- supervisor.slots.port.cpu.weight: 1.2
- supervisor.slots.port.mem.weight: 0.7
- supervisor.slots.ports: null	

配置每个supervisor下可用于启动woker的slot的方式有2种：

1. 手动指定

 		supervisor.slots.ports:
			- 6800
			- 6801
			- 6802
			- 6803
 
2. 自动计算

	下面四个配置项都是可选配置：

		# if supervisor.slots.ports is null,
		# the port list will be generated by cpu cores and system memory size
		# for example,
		# there are cpu_num = system_physical_cpu_num/supervisor.slots.port.cpu.weight
		# there are mem_num = system_physical_memory_size/(worker.memory.size * supervisor.slots.port.mem.weight)
		# The final port number is min(cpu_num, mem_num)
		supervisor.slots.ports.base: 6800
		supervisor.slots.port.cpu.weight: 1.2
		supervisor.slots.port.mem.weight: 0.7
		supervisor.slots.ports: null	

### 启用ZeroMQ ###
要启用ZeroMQ作为JStorm底层的消息传输插件，除了安装ZeroMQ和JZMQ以外，还需要配置：

- storm.messaging.transport: 默认值是"com.alibaba.jstorm.message.netty.NettyContext"，要改为"com.alibaba.jstorm.message.zeroMq.MQContext"
- java.library.path: Zeromq 和java zeromq library的安装目录，默认"/usr/local/lib:/opt/local/lib:/usr/lib"

### storm.xml样例 ###
	########### These MUST be filled in for a storm configuration
	 storm.zookeeper.servers:
	     - "node3"
	
	 storm.zookeeper.root: "/jstorm"
	
	# cluster.name: "default"
	
	 #nimbus.host/nimbus.host.start.supervisor is being used by $JSTORM_HOME/bin/start.sh
	 #it only support IP, please don't set hostname
	 # For example
	 # nimbus.host: "10.132.168.10, 10.132.168.45"
	 nimbus.host: "10.0.0.102"
	 #nimbus.host.start.supervisor: false
	 nimbus.childopts: "-Xms512m -Xmx512m"
	
	# %JSTORM_HOME% is the jstorm home directory
	 storm.local.dir: "%JSTORM_HOME%/data"
	 # please set absolute path, default path is JSTORM_HOME/logs
	# jstorm.log.dir: "absolute path"
	
	# java.library.path: "/usr/local/lib:/opt/local/lib:/usr/lib"
	
	
	
	# if supervisor.slots.ports is null,
	# the port list will be generated by cpu cores and system memory size
	# for example,
	# there are cpu_num = system_physical_cpu_num/supervisor.slots.port.cpu.weight
	# there are mem_num = system_physical_memory_size/(worker.memory.size * supervisor.slots.port.mem.weight)
	# The final port number is min(cpu_num, mem_num)
	# supervisor.slots.ports.base: 6800
	# supervisor.slots.port.cpu.weight: 1.2
	# supervisor.slots.port.mem.weight: 0.7
	# supervisor.slots.ports: null
	# supervisor.slots.ports:
	#    - 6800
	#    - 6801
	#    - 6802
	#    - 6803
	
	# Default disable user-define classloader
	# If there are jar conflict between jstorm and application,
	# please enable it
	# topology.enable.classloader: false
	
	# enable supervisor use cgroup to make resource isolation
	# Before enable it, you should make sure:
	#       1. Linux version (>= 2.6.18)
	#       2. Have installed cgroup (check the file's existence:/proc/cgroups)
	#       3. You should start your supervisor on root
	# You can get more about cgroup:
	#   http://t.cn/8s7nexU
	# supervisor.enable.cgroup: false
	
	
	### Netty will send multiple messages in one batch
	### Setting true will improve throughput, but more latency
	# storm.messaging.netty.transfer.async.batch: true
	
	### if this setting  is true, it will use disruptor as internal queue, which size is limited
	### otherwise, it will use LinkedBlockingDeque as internal queue , which size is unlimited
	### generally when this setting is true, the topology will be more stable,
	### but when there is a data loop flow, for example A -> B -> C -> A
	### and the data flow occur blocking, please set this as false
	# topology.buffer.size.limited: true
	
	### default worker memory size, unit is byte
	# worker.memory.size: 2147483648
	
	# Metrics Monitor
	# topology.performance.metrics: it is the switch flag for performance
	# purpose. When it is disabled, the data of timer and histogram metrics
	# will not be collected.
	# topology.alimonitor.metrics.post: If it is disable, metrics data
	# will only be printed to log. If it is enabled, the metrics data will be
	# posted to alimonitor besides printing to log.
	# topology.performance.metrics: true
	# topology.alimonitor.metrics.post: false
	
	# UI MultiCluster
	# Following is an example of multicluster UI configuration
	# ui.clusters:
	#     - {
	#         name: "jstorm",
	#         zkRoot: "/jstorm",
	#         zkServers:
	#             [ "localhost"],
	#         zkPort: 2181,
	#       }

## 其他 ##
可以通过设置JSTORM\_CONF\_DIR 自定义 JStrom 配置文件目录的路径
